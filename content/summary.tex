% only the text for the summary

Il presente lavoro di tesi si concentra sul design e l'implementazione di \textbf{Goccia}, una libreria ad alte prestazioni per la costruzione di \emph{data pipeline} nel linguaggio Go. Go è un linguaggio di programmazione compilato sviluppato da Google nel 2009, caratterizzato da un modello di concorrenza basato su \emph{goroutine} e \emph{channel} che segue il principio ``do not communicate by sharing memory; share memory by communicating''. Le goroutine sono unità leggere di esecuzione gestite dal runtime che consentono di scrivere codice concorrente efficiente, mentre i channel forniscono primitive di comunicazione sicure tra goroutine. Il linguaggio include inoltre supporto nativo per profiling, osservabilità e pacchetti essenziali per reti e operazioni atomiche, costituendo fondamenta robuste per la progettazione di infrastrutture di \emph{data processing} ad alte prestazioni.

Nel contesto dei sistemi di elaborazione dati, una pipeline è una sequenza di stadi attraverso cui i messaggi fluiscono in modo ordinato, trasformandosi progressivamente da una rappresentazione grezza a una forma più strutturata e significativa dal punto di vista del dominio applicativo. Ogni stage ha responsabilità ben delimitata e definita: riceve input da stage precedenti o da sorgenti esterne, applica una trasformazione specifica, e produce un output che diventa l'ingresso dello stage successivo. Questa decomposizione architettonica promuove modularità, separazione delle responsabilità, riusabilità e facilita la comprensione e il testing di ciascun componente isolatamente. Dal punto di vista della concorrenza, il modello pipeline consente un notevole beneficio di throughput: ogni stage può operare in parallelo sugli elementi che riceve. Concretamente, mentre lo stage 1 elabora il messaggio N, lo stage 2 può già processare il messaggio N-1, e lo stage 3 il messaggio N-2. Questo parallelismo stadio-a-stadio incrementa significativamente il throughput complessivo del sistema rispetto a un'elaborazione sequenziale: invece di attendere che ogni messaggio attraversi tutta la catena prima di processarne uno nuovo, il sistema mantiene un ``pipelining'' attivo dove più messaggi sono elaborati contemporaneamente in stadi diversi. Un aspetto centrale riguarda i meccanismi di collegamento tra stage, poiché devono garantire sia la sicurezza rispetto alla concorrenza che la capacità di assorbire variazioni locali di carico. In particolare, l'impiego di buffer intermedi consente di mitigare la \emph{backpressure}, il fenomeno per cui uno stage più lento a valle rallenterebbe la produzione di uno stage più veloce a monte se non vi fossero buffer di accumulo. Un buffer consente temporaneamente di disaccoppiare la velocità di produzione da quella di consumo, riducendo l'impatto di variabilità di carico sulla latenza.

L'architettura di \textbf{Goccia} si fonda su due componenti essenziali: \emph{Stage} e \emph{Connector}. Ogni stage implementa tre metodi per gestirne il \emph{lifecycle}: \texttt{Init} per l'inizializzazione delle risorse, \texttt{Run} per l'elaborazione dei dati, e \texttt{Close} per il rilascio delle risorse. Gli stage vengono eseguiti in almeno una goroutine dedicata, permettendo l'esecuzione parallela degli stadi della pipeline. Gli stage sono categorizzati in tre gruppi principali: \emph{Ingress Stage}, che rappresentano i punti di ingresso dei dati nella pipeline; \emph{Processor Stage}, che implementano trasformazioni modulari sui messaggi; ed \emph{Egress Stage}, che rappresentano i punti di uscita della pipeline, persistendo o trasmettendo messaggi verso destinazioni esterne. Un aspetto distintivo di \textbf{Goccia} riguarda l'implementazione dei \emph{Connector}, ovvero i buffer frapposti tra gli stage. La libreria utilizza \emph{ring buffer lock-free}, in luogo dei \emph{channel} nativi di Go. Questa scelta progettuale mira a massimizzare il throughput in scenari ad alto carico, implementando tre varianti: SPSC (\emph{Single Producer Single Consumer}), utilizzato per la comunicazione tra stage consecutivi; SPMC (\emph{Single Producer Multiple Consumer}), impiegato per operazioni di \emph{fan-out}; e MPSC (\emph{Multiple Producer Single Consumer}), per operazioni di \emph{fan-in}.

L'implementazione dei ring buffer si basa su operazioni atomiche \emph{Compare-and-Swap} (CAS), che consentono di modificare una locazione di memoria in modo atomico a livello CPU, eliminando condizioni di \emph{data race} senza ricorrere a lock espliciti. Un elemento cruciale è il padding delle cache line tramite \texttt{cpu.CacheLinePad}. Nelle architetture x86-64, le cache line hanno dimensione di 64 byte: quando la CPU accede a un dato in memoria, l'intero blocco di 64 byte contenente quel dato viene caricato nella cache locale del core. In contesti concorrenti, questo può generare il fenomeno del \emph{false sharing}, che si verifica quando due thread accedono a variabili distinte che però risiedono nella stessa cache line: il protocollo di coerenza della cache costringe l'intera cache line a essere invalidata e ricaricata ogni volta che uno dei due thread la modifica, anche se le variabili non condividono logicamente dati. Separando \texttt{head} e \texttt{tail} tramite padding, si garantisce che i due indici risiedano su cache line diverse, eliminando la contesa e migliorando il throughput di un ordine di grandezza in scenari di alta contesa multicore. Le operazioni di enqueue e dequeue adottano una strategia ibrida: inizialmente tentano l'operazione con spinning limitato; quindi, si bloccano su una \emph{condition variable} se il buffer rimane pieno o vuoto, combinando l'efficienza del \emph{busy-waiting} per brevi contese con il blocco efficiente per periodi prolungati.

Gli \emph{Ingress Stage} rappresentano i punti di ingresso dei dati nella pipeline. \textbf{Goccia} fornisce diverse implementazioni: \emph{Ticker Stage}, che genera messaggi periodici sfruttando \texttt{time.Ticker}, utile per testing e benchmarking; \emph{UDP Stage}, che riceve datagrammi UDP dalla rete, implementando una strategia di buffering con size configurabile e utilizzando un \texttt{sync.Pool} per il riuso della memoria, riducendo la pressione sul \emph{garbage collector}; \emph{TCP Stage}, che gestisce connessioni TCP persistenti, supportando due modalità di framing (\emph{Delimited} e \emph{Length-Prefixed}) e implementando un'architettura multi-goroutine, con una goroutine principale che accetta connessioni e una goroutine dedicata per ciascun client connesso; \emph{Kafka Stage}, che consuma messaggi da topic Kafka, integrandosi con architetture \emph{event-driven}; \emph{eBPF Stage}, che si collega alla map ring buffer dei programmi eBPF, permettendo l'acquisizione di eventi di basso livello come syscall e pacchetti di rete; e \emph{File Stage}, che legge dati da file su disco, supportando diverse modalità di lettura.

I \emph{Processor Stage} rappresentano unità di elaborazione intermedia, implementando trasformazioni modulari sui messaggi. Esempi significativi includono \emph{Cannelloni Decoder/Encoder}, specializzati per la serializzazione e deserializzazione di messaggi CAN nel formato Cannelloni, uno standard di incapsulamento per il trasporto di messaggi CAN su reti IP; \emph{CAN Processor}, che decodifica messaggi CAN grezzi in strutture tipizzate; \emph{Filter Stage}, che filtra messaggi in base a predicati definiti dall'utente; \emph{Tee Stage}, che duplica il flusso di messaggi verso molteplici connettori di output, abilitando il pattern \emph{fan-out}; \emph{Reorder Buffer} (ROB), che riordina messaggi fuori sequenza in base ad un sequence number e ne stima il timestamp tramite il metodo di Holt per il \emph{Double Exponential Smoothing}; e \emph{Custom Stage}, che consente agli utenti di implementare logica di elaborazione arbitraria tramite l'implementazione di un'interfaccia personalizzata \texttt{CustomHandler}, fornendo un'astrazione generica che delega completamente la logica di processing all'utente.

Il metodo di Holt, introdotto nel 1957, estende il \emph{simple exponential smoothing} per permettere la previsione e l'analisi di serie temporali che presentano un andamento di tendenza lineare. Mentre il \emph{simple exponential smoothing} si limita a modellare una componente di livello costante e risulta inadatto a serie con trend manifesto, il metodo di Holt introduce una componente aggiuntiva dedicata alla stima della tendenza (slope), rendendo il modello maggiormente adatto all'elaborazione di dati con comportamento non stazionario. Nel contesto di \textbf{Goccia}, il metodo di Holt è impiegato per stimare e correggere il timestamp di messaggi ricevuti, in particolare quando i messaggi arrivano fuori ordine o affetti da jitter. Il metodo fornisce un meccanismo efficiente per separare la componente di livello (il timestamp smussato, depurato dal jitter) dalla componente di trend (il rate di crescita sistematico dei receive time). Poiché i timestamp di ricezione seguono un trend lineare crescente, il metodo di Holt è particolarmente adatto a catturare questa dinamica e a fornire stime robuste nonostante le fluttuazioni dovute al jitter di rete.

Gli \emph{Egress Stage} rappresentano i punti di uscita della pipeline, persistendo o trasmettendo messaggi verso destinazioni esterne. \textbf{Goccia} fornisce diverse implementazioni: \emph{UDP/TCP Egress}, che inviano messaggi verso endpoint remoti via protocolli UDP o TCP; \emph{Kafka Egress}, che pubblica messaggi su topic Kafka, con supporto per tracciamento distribuito tramite propagazione del ``contesto'' OpenTelemetry negli header dei messaggi di Kafka; \emph{File Egress}, che persiste messaggi su disco in modalità \emph{append-only}, con buffering configurabile e flushing basato su soglie di riempimento o deadline temporali; \emph{QuestDB Egress}, che inserisce dati in QuestDB, un database \emph{time-series} colonnare progettato specificamente per la gestione di flussi di dati indicizzati temporalmente, gestendo automaticamente pooling di connessioni e flushing; e \emph{Sink Egress}, che consuma e distrugge messaggi senza persistenza, utilizzato per testing e benchmarking.

\textbf{Goccia} implementa un \emph{Worker Pool} per la gestione dinamica della concorrenza, composto da code di \emph{fan-out} (distribuzione task ai worker), code di \emph{fan-in} (raccolta risultati), uno \emph{Scaler} per l'autoscaling, mentre i \emph{Worker} per l'elaborazione effettiva. Lo \emph{Scaler} implementa un algoritmo adattivo che regola il numero di worker attivi in funzione della profondità della coda, con parametri configurabili. La riduzione dei worker (scale-down) adotta un meccanismo di \emph{exponential backoff} per prevenire oscillazioni eccessive. Il \emph{Worker Pool} è utilizzato sia dai \emph{Processor Stage} che dagli \emph{Egress Stage} quando configurati in ``running mode'' \emph{Worker Pool}, garantendo scalabilità dinamica e throughput elevato tramite meccanismi di \emph{fan-out}, \emph{fan-in} e autoscaling adattivo.

Il quinto capitolo della tesi descrive l'implementazione di un sistema di telemetria automobilistica per SquadraCorse, che rappresenta il principale caso d'uso che ha motivato lo sviluppo di \textbf{Goccia}. Il sistema è organizzato in layer distinti: \emph{Ingestion Layer}, che acquisisce dati CAN dal bus del veicolo via UDP, utilizzando il protocollo Cannelloni; \emph{Processing Layer}, che utilizza effettivamente la libreria \textbf{Goccia}, decodifica i messaggi CAN, applica correzioni di timestamp e riordina i messaggi; \emph{Storage Layer}, che persiste i dati telemetrici in QuestDB per analisi real-time; \emph{Observability Layer}, che integra OpenTelemetry per tracciamento distribuito, raccogliendo metriche e log strutturati; e \emph{Visualization Layer}, che presenta i dati tramite dashboard Grafana, permettendo il monitoraggio in tempo reale di pressioni, tensioni, temperature e stati delle macchine a stati finiti del veicolo.

Il capitolo 6 delinea le direzioni di evoluzione futura della libreria, includendo il supporto a nuovi protocolli di trasporto (QUIC per comunicazioni a bassa latenza, MQTT per IoT, FastHTTP per servizi REST ad alte prestazioni), l'implementazione di pattern avanzati (\emph{Request-Reply} con \emph{Futures}, \emph{Content-Based Routing}), l'estendibilità con \emph{Custom Ingress/Egress} e supporto a InfluxDB, e il miglioramento della test coverage.

\textbf{Goccia} rappresenta una soluzione completa per la costruzione di \emph{data pipeline} ad alte prestazioni in Go, combinando i vantaggi del modello di concorrenza di Go con tecniche avanzate di sincronizzazione \emph{lock-free}. L'architettura modulare e l'impiego di ring buffer ottimizzati consentono di raggiungere throughput elevati in scenari multicore, mentre l'integrazione nativa con OpenTelemetry garantisce osservabilità completa. Il caso d'uso di SquadraCorse dimostra l'applicabilità della libreria in contesti real-time, dove l'elaborazione di dati telemetrici richiede latenze ridotte e affidabilità. Gli sviluppi futuri mirano a estendere le capacità della libreria con nuovi protocolli e pattern architetturali, consolidando \textbf{Goccia} come framework di riferimento per \emph{data pipeline} in Go.


% $400\times$ is nicer than 400x
