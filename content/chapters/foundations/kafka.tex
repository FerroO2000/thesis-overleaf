\section{Apache Kafka}

Apache Kafka è una piattaforma di streaming distribuito nata originariamente in LinkedIn e successivamente resa open source, progettata per fungere da log distribuito ad alto throughput e da sistema di messaggistica publish/subscribe scalabile e fault-tolerant~~\cite{kafka:docs}. Concettualmente, Kafka organizza i dati in topic, a loro volta suddivisi in partizioni: ciascuna partizione è implementata come un log append-only ordinato, memorizzato su disco, in cui i record sono identificati da un offset progressivo. Questa astrazione di log distribuito consente di gestire in modo naturale flussi di eventi temporali, come stream di dati, metriche o log applicativi.

I produttori (producer) scrivono messaggi in append nelle partizioni dei topic, mentre i consumatori (consumer) leggono sequenzialmente i messaggi mantenendo localmente il proprio offset di lettura. La separazione tra il log persistente sul broker e lo stato di consumo lato client permette di avere molteplici consumer group che rileggono gli stessi dati in momenti diversi. Kafka garantisce, per ogni partizione, l'ordinamento totale dei messaggi, mentre la scalabilità orizzontale è ottenuta distribuendo le partizioni su un cluster di broker con meccanismi di replica per la tolleranza ai guasti.

Dal punto di vista dello storage, ogni partizione è fisicamente rappresentata da una directory che contiene una sequenza di segmenti di log (file .log) e relativi indici (offset index, time index). Quando un segmento raggiunge una certa dimensione o anzianità, viene ``ruotato'' e ne viene creato uno nuovo. Questa organizzazione facilita sia le politiche di retention (delete o compact) sia l'accesso sequenziale ad alta efficienza, poiché le letture dei consumer si concentrano spesso sui segmenti più recenti.

Per quanto riguarda le garanzie di consegna, Kafka supporta diverse semantiche (at-most-once, at-least-once, exactly-once) a seconda della configurazione dei producer, dei consumer e delle applicazioni di stream processing a valle. L'integrazione con sistemi come Kafka Streams, Flink o Spark Streaming consente di costruire pipeline di elaborazione stateful strettamente integrate con il log sottostante. In contesti di stream processing, Kafka viene spesso utilizzato come buffer tra sorgenti rumoose/ad alto volume (sensori, microservizi, gateway di bordo) e sistemi di persistenza o analisi batch, permettendo l'implementazione di architetture scalabili e resilienti.